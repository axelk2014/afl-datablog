{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A walk in the park - the sourcing, cleaning and processing edition\n",
    "\n",
    "This is a companion notebook to \"A walk in the park\" (link here) detailing all the major steps and tools in sourcing and cleaning the data to make it suitable for visualisation and analysis. This notebook is structure in a similar way, broken up into the 3 major questions.\n",
    "\n",
    "1. [Data](#data)\n",
    "1. [How this notebook works](#howthis)\n",
    "1. [Initial setup, sourcing, and cleaning](#setup)\n",
    "1. [Question 1: Are there any significant differences between teams, looking only at the game stats? ](#question-1)\n",
    "1. [Question 2: Does the average temperature affect team performance?](#question-2)\n",
    "1. [Question 3: Is there a different body type between professional codes?](#question-3)\n",
    "1. [Data sources](#data-sources)\n",
    "1. [Installed packages and technology](#packages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"howthis\">How this notebook works:</a>\n",
    "This notebook assumes a moderate working knowledge of R, and as such I won't be parsing each line in my code, but pulling out 'highlights'. Most of the code in this noteboook is in non-executable code formatted blocks. Full executable code is available in the git repo here.\n",
    "\n",
    "#### <a id=\"data\">Data:</a>\n",
    "Some of the raw datafiles will be made available in the repo. For other data (football players, soccer, NFL, NRL) you will need to run the sourcing/scraping scripts to download the data.\n",
    "\n",
    "\n",
    "### <a id=\"setup\">Initial setup, sourcing, and cleaning:</a>\n",
    "\n",
    "Setting up R project:\n",
    "Taking inspiration from \"Nice R Code\" ([link][1]), I initially created 3 R files:\n",
    "1. **analysis-function.R** - containing all the the major analysis code. The top of the file looks like this:\n",
    "```\n",
    "library(RCurl)\n",
    "library(stringr)\n",
    "library(plyr)\n",
    "library(dplyr)\n",
    "...\n",
    "setwd(\"/Users/alex/Documents/datasets/afl\")\n",
    "source(\"src/data_clean.R\")\n",
    "setwd(\"/Users/alex/Documents/datasets/afl\")\n",
    "source(\"src/functions.R\")\n",
    "```\n",
    "This enables me to quickly run all three files by executing (or 'sourcing') the first file. Centralising the list of required packages also enables me to track and therefore easily upgrade the packages when required (R went through a major update since I started this project, as did ggplot. So I had to re-install all packages twice)<br><br>\n",
    "2. **data_clean.R** - contains all the code required to clean the data into dataframes for the other scripts to use. <br><br>\n",
    "3. **functions.R** - contains all the functions used in analysis code. Some of them required to generate multiple plots and setup global variables.\n",
    "\n",
    "I also created the following file **plotlyscript.R**, as I was using principally ggplot and base R libraries to plot the data, and I wanted to separate my plotly code from the rest of the project.\n",
    "\n",
    "\n",
    "#### Directory structure:\n",
    "```\n",
    "afl/\n",
    "|── src/\n",
    "|── data/\n",
    "|── figs/\n",
    "|── output/\n",
    "|── references/\n",
    "```\n",
    "\n",
    "* The ``src`` directory contains all the R, Python and bash scripts used in the project\n",
    "* The ``data`` directory contains all the raw data used in this project, including downloaded webpages, csvs and databases. This directory is used as 'read-only' to preserve the integrity of the data\n",
    "* The ``figs`` directory where all saved plots and charts are stored\n",
    "* The ``output`` directory where any processed data and dataframes are stored\n",
    "* The ``references`` directory where I downloaded reference JPEGs for AFL team logos and colours\n",
    "\n",
    "\n",
    "#### Technology stack:\n",
    "See platform and packages here: [Installed packages and technology](#packages)\n",
    "\n",
    "[1]: https://nicercode.github.io/blog/2013-04-05-projects/ \"Nice R code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"question-1\"> Question 1: Are there any significant differences between teams, looking only at the game stats?</a>\n",
    "\n",
    "Using R to download the file that will be used for the main data set I run:\n",
    "```\n",
    "download.file(\"http://www.aussportsbetting.com/historical_data/afl.xlsx\",\"data/raw/afl.xlsx\",method=\"curl\")\n",
    "```\n",
    "In subsequent re-runs of the data_clean script I would comment that line out, and use the already downloaded xlsx  (Ie I don't need to download it each time).\n",
    "\n",
    "Using ``in2csv`` from the bash ``csvkit`` toolset, I convert the afl.xlsx into a csv file. \n",
    "```\n",
    "system(\"in2csv data/raw/afl.xlsx > data/raw/afl.csv\")\n",
    "```\n",
    "*I find that to create R dataframes, CSVs are the best file format to use. Also, if I need to run any further processing in bash or python, CSVs are the better option - thus I did not consider read.xlsx or similar R methods*\n",
    "\n",
    "As the script/project grows, the duration of the file and data processing extends, so I add console feedback to call out completion of some sections. \n",
    "\n",
    "To time the data cleaning process I also enclose the code as follows:\n",
    "```\n",
    "a1 <- Sys.time()\n",
    "...\n",
    "...\n",
    "a2 <- Sys.time()\n",
    "print(a2 -a1)\n",
    "print(\"functions.R & data_clean.R sourcing complete\") \n",
    "```\n",
    "R automatically works out the human readable time with this line ``print(a2-a1)`` and prints out something like\n",
    "```\n",
    "> print(a2 -a1)\n",
    "Time difference of 5.701957 mins\n",
    "```\n",
    "The first analysis required a comparison between goals and behinds. In Aussie rules footbal (AFL), a goal is worth 6 points and a behind is worth a single point, however for this analysis the total number of points scored was not important, I wanted to compare frequency of goals and behinds kicked. \n",
    "\n",
    "Whilst the dataset does contain the following variables: \"Home.Goals\",\"Home.Behinds\",\"Away.Goals\",\"Away.Behinds\", I need to extract the counts by team. I do this by:\n",
    "1. create a vector ``teams`` with a unique list of all the teams, and then \n",
    "1. expand that to two vectors that include \"[Team].Goals\", and \"[Team].Behinds\" for all teams.\n",
    "1. use each vector to create new variables in the dataset\n",
    "1. fill with NAs\n",
    "   \n",
    "Code snippet:\n",
    "\n",
    "```\n",
    "teams <- unique(tmp.afl$Away.Team)\n",
    "teams <- gsub(\" \",\".\", teams)\n",
    "\n",
    "teams.goals <- paste(teams, \".Goals\", sep=\"\")\n",
    "teams.behinds <- paste(teams, \".Behinds\", sep=\"\")\n",
    "\n",
    "tmp.afl[,teams.goals] <- NA\n",
    "tmp.afl[,teams.behinds] <- NA\n",
    "```\n",
    "\n",
    "Now populate the new variables using a loop that:\n",
    "1. First identifies all rows with a specific team name\n",
    "1. Then identifies which variable (column) in the data set has the same name\n",
    "1. Populates the column with the data from the respective \"Home.Goals\" and \"Home.Behinds\" columns\n",
    "1. Wash, rinse repeat with a second loop that does exactly the same with \"Away.Goals\" and \"Away.Behinds\"\n",
    "\n",
    "Code snippet:\n",
    "\n",
    "```\n",
    "for ( teamname in unique(tmp.afl$Home.Team) ) {\n",
    "  \n",
    "  RowIndex = which(tmp.afl$Home.Team == teamname)\n",
    "  # find ncol index of appropriate columns\n",
    "  ColIndex = which( substr(names(tmp.afl),1,nchar(teamname)) %in% gsub(\" \",\".\", teamname))\n",
    "  \n",
    "  # Home Goals\n",
    "  tmp.afl[RowIndex, ColIndex[1]] <- tmp.afl[RowIndex,9 ]\n",
    "  # Home Behinds\n",
    "  tmp.afl[RowIndex, ColIndex[2]] <- tmp.afl[RowIndex,10 ]\n",
    "  \n",
    "}\n",
    "```\n",
    "I experimented with using ``ldply`` and other ``plyr`` methods, but for the sake of readability and easier debugging, I used a ``for`` loop. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"question-2\">Question 2: Does the average temperature affect team performance?</a>\n",
    "#### (i.e do some teams play better at certain temperatures?)\n",
    "\n",
    "\n",
    "#### Method:\n",
    "**Sourcing the data:**\n",
    "1. Detail the physical location of each stadium using wikipedia.\n",
    "2. Using bom.gov.au and google maps, locate the weather stations closest to each stadium\n",
    "3. Download the weather records from selected weather stations (http://www.bom.gov.au/climate/data/index.shtml?bookmark=201)\n",
    "4. Some of the weather records are incomplete, so work out secondary stations that are still near the stadiums that can fill in the gaps\n",
    "\n",
    "**Cleaning the data**\n",
    "<i> Note: not all steps for data cleaning are detailed, just highlights. </i>\n",
    "\n",
    "Assuming you have extracted all the bom zip files csvs into a single directory:\n",
    "1. Create a data frame with all weather records (ldply is our friend):<br>\n",
    "```\n",
    "lstofCSVs <- as.list(list.files(\"data/raw/bomdata\"))\n",
    "tmpcsv <- ldply(paste(\"data/raw/bomdata/\",lstofCSVs,sep=\"\"), read.csv)\n",
    "```\n",
    "```\n",
    "head(tmpcsv[which(tmpcsv$Date>'2009-10-01'),], 2)\n",
    "      Product.code Bureau.of.Meteorology.station.number Year Month Day\n",
    "15616   IDCJAC0009                                 9151 2009    10   2\n",
    "15617   IDCJAC0009                                 9151 2009    10   3\n",
    "      Rainfall.amount..millimetres.\n",
    "15616                           0.0\n",
    "15617                           2.2\n",
    "      Period.over.which.rainfall.was.measured..days. Quality\n",
    "15616                                             NA       Y\n",
    "15617                                              1       Y\n",
    "      Maximum.temperature..Degree.C.\n",
    "15616                             NA\n",
    "15617                             NA\n",
    "      Days.of.accumulation.of.maximum.temperature\n",
    "15616                                          NA\n",
    "15617                                          NA\n",
    "      Minimum.temperature..Degree.C.\n",
    "15616                             NA\n",
    "15617                             NA\n",
    "      Days.of.accumulation.of.minimum.temperature       Date\n",
    "15616                                          NA 2009-10-02\n",
    "15617                                          NA 2009-10-03\n",
    "```\n",
    "\n",
    "1. Create a ISO 8601 formatted date field (helps with dataframe operations later on)<br><br>\n",
    "``tmpcsv$Date <- as.Date(paste(tmpcsv$Year, \"-\", sprintf(\"%02d\", tmpcsv$Month),\n",
    "\"-\",sprintf(\"%02d\",tmpcsv$Day), sep=\"\"))\n",
    "``<br><br>\n",
    "\n",
    "1. Get list of dates of all games played (2016 only)<br><br>\n",
    "``lstDatesofGamesPlayed <- unique(tmp.afl[which(tmp.afl$Date < \"2017-01-01\"),]$Date)``<br><br>\n",
    "\n",
    "1. Create a frequency table of all city climate records <br><br>\n",
    "``vecCities <- sapply(split(lstofdftemp.dates, lstofdftemp.dates$City),nrow)``<br><br>\n",
    "\n",
    "1. loop through cities and where the frequency count is less than the number of dates of games played, get the records from secondary priority stations, and then collapse the dataframe to get rid of NAs<br>\n",
    "\n",
    "    ``df.temp.missing <- setDT(df.temp.missing)[,lapply(.SD, na.omit), by=Date]``\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"question-3\">Question 3: Is there a different body type between professional codes?</a>\n",
    "#### (and how different?)\n",
    "\n",
    "### AFL player data source:\n",
    "** Python script - (downloadplayerpages.py)**\n",
    "\n",
    "First source the appropriate AFL player stats from here (http://afltables.com/afl/stats/playersA_idx.html), using a python script to download the pages using ``urllib.request`` and to parse them using ``BeautifulSoup``. I initially attempted to use ``scrapy`` but found it too convoluted for what I wanted to achieve. \n",
    "\n",
    "For each iteration succesful iteration of the loop, there is a ``time.sleep(4)`` which pauses the program execution for 4 seconds before looping again. I do this as a matter of courtesy  to those that run the website.\n",
    "\n",
    "The loop select player pages whose first year of playing is >= 2009 (start of AFL fixtures dataset) or the last year played >= 2009 - this helps cut down the number of pages to download.\n",
    "\n",
    "**R script - (data_clean.R)**\n",
    "\n",
    "Using R, I loop through all the downloaded player pages, and extract the relevant data (DOB, height,weight)\n",
    "\n",
    "```\n",
    "  doc <- htmlTreeParse(paste(\"data/raw/afltables.com/\", player, sep=\"\"), useInternalNodes  = TRUE)\n",
    "  plain.text <- xpathSApply(doc, \"/html/body/center/text()\", xmlValue)\n",
    "  tblteam <- as.data.frame(readHTMLTable(doc)[1])\n",
    "  \n",
    "   # DOB\n",
    "  dob <- plain.text[5]\n",
    "  # height\n",
    "  hght <- plain.text[8]\n",
    "  # weight\n",
    "  wght <- plain.text[9]\n",
    "  \n",
    "```\n",
    "        \n",
    "### NFL source\n",
    "\n",
    "* Download this html page (http://www.nfl.com/teams)\n",
    "* The following bash script is executed in the same directory as the source- the output being a list of links to each team roster page<br>\n",
    "\n",
    "```\n",
    "grep -e 'http:\\/\\/www.nfl.com\\/teams\\/[a-z]*\\/profile?' NFL\\ Teams.htm > nfllinks.txt\n",
    "cat nfllinks.txt | gawk '{FS=\"=\\\"\"}{ print $3}' | sed 's/onclick//g' > nflink2.txt\n",
    "nflteams=($(cat nfllinks2.txt `| gawk '{FS=\"/\"}{ print $5\",\"substr($6,14,3) }' | uniq))\n",
    "\n",
    "for i in \"${nflteams[@]}\"; do echo \"http://www.nfl.com/teams/\"${i%????}\"/roster?team=\"${i: -3}; done\n",
    "```\n",
    "\n",
    "* using the text file with all the links, I manually copy/paste the URL into a GoogleSheets formula:<br>\n",
    "``=IMPORTHTML(\"http://www.nfl.com/teams/seattleseahawks/roster?team=SEA\",\"table\",0)``<br>\n",
    "which automatically imports a nice clean table with all player stats, which I then copy paste to one CSV.\n",
    "* then the csv is imported into R with ``read.csv``\n",
    "* imperial weights and heights are converted into metric\n",
    "\n",
    "### NRL source\n",
    "* Download the html file from (https://www.zerotackle.com/rugby-league/players/)\n",
    "* Run this python script to create a text file that extracts every NRL player name and creates a text document with one player URL per line\n",
    "\n",
    "```\n",
    "import os,re,time\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(open(\"NRLPlayers.htm\"),\"lxml\")\n",
    "\n",
    "playerlinks = soup.find_all(\"a\")\n",
    "mymatch = re.compile('[/]rugby-league[/]players[/][a-z]*-[a-z]*[/]')\n",
    "players = mymatch.findall(str(playerlinks))\n",
    "f = open('output.txt','w')\n",
    "for p in players:\n",
    "\tf.write(p+\"\\n\")\n",
    "```\n",
    "\n",
    "* run this bash code to download all the html pages into a directory:\n",
    "``$ cat output.txt | xargs -n1 curl -O``\n",
    "* Then run this R code in the directory to create a dataframe with the NRL player data\n",
    "\n",
    "```\n",
    "dfPlayers.NRL <- data.frame(player=character(),\n",
    "                        dob=character(),\n",
    "                        height=character(),\n",
    "                        weight=character(),\n",
    "                        year=character(),\n",
    "                        team=character())\n",
    "\n",
    "for ( f in list.files(path = \"data/NRLplayers/\", pattern = \"index*\")) {\n",
    "  \n",
    "  print(f)\n",
    "  \n",
    "  doc <- htmlTreeParse(paste(\"data/NRLplayers/\",f,sep=\"\"), useInternalNodes  = TRUE)\n",
    "  player.name <- xpathSApply(doc, \"//*[@id=\\\"td-outer-wrap\\\"]/div[2]/div/div/div/h1/text()\", xmlValue)\n",
    "  tblteam <- as.data.frame(readHTMLTable(doc)[1])\n",
    "  \n",
    "  tmpdf <- cbind(player=player.name, \n",
    "                 dob=as.character(tblteam[4,2]), \n",
    "                 height=as.character(tblteam[3,2]), \n",
    "                 weight=as.character(tblteam[2,2]),\n",
    "                 year=\"2017\", \n",
    "                 team=as.character(tblteam[11,1]))\n",
    "  \n",
    "  dfPlayers.NRL <- rbind(dfPlayers.NRL,tmpdf)\n",
    "  \n",
    "  tmpdf <- NULL\n",
    "  player.name <- NULL\n",
    "  doc <- NULL\n",
    "  tblteam <- NULL\n",
    "  \n",
    "}\n",
    "```\n",
    "\n",
    "* Additional code in the file cleans it up and converts it to a nice standardised dataset.\n",
    "\n",
    "\n",
    "### Soccer\n",
    "This one was easy, and interesting. The data source is an SQLlite db downloaded from a Kaggle nugget (see [data sources](#data-sources) for link).\n",
    "After downloading it, I run the following R code to create the dataset, and convert the weight into metric.\n",
    "\n",
    "```\n",
    "setwd(\"data/soccer/\")\n",
    "con = dbConnect(RSQLite::SQLite(), dbname=\"database.sqlite\")\n",
    "# get a list of all tables\n",
    "alltables = dbListTables(con)\n",
    "dfPlayers.soccer = dbGetQuery( con,'select * from Player' )\n",
    "\n",
    "dfPlayers.soccer$weight <- round(dfPlayers.soccer$weight*0.453592,2)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"packages\">Installed packages and technology stack</a>\n",
    "\n",
    "R Session Info:\n",
    "```\n",
    "R version 3.4.0 (2017-04-21)\n",
    "Platform: x86_64-apple-darwin15.6.0 (64-bit)\n",
    "Running under: OS X El Capitan 10.11.6\n",
    "\n",
    "Matrix products: default\n",
    "BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib\n",
    "LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib\n",
    "\n",
    "locale:\n",
    "[1] en_AU.UTF-8/en_AU.UTF-8/en_AU.UTF-8/C/en_AU.UTF-8/en_AU.UTF-8\n",
    "\n",
    "attached base packages:\n",
    "[1] grid      stats     graphics  grDevices utils     datasets  methods  \n",
    "[8] base     \n",
    "\n",
    "other attached packages:\n",
    " [1] webshot_0.4.0       htmltools_0.3.6     knitr_1.15.1       \n",
    " [4] pander_0.6.0        readr_1.1.0         RColorBrewer_1.1-2 \n",
    " [7] plotly_4.6.0        formattable_0.2.0.1 DBI_0.6-1          \n",
    "[10] gridExtra_2.2.1     rvest_0.3.2         xml2_1.1.1         \n",
    "[13] XML_3.98-1.6        data.table_1.10.4   reshape_0.8.6      \n",
    "[16] reshape2_1.4.2      ggplot2_2.2.1       plyr_1.8.4         \n",
    "[19] stringr_1.2.0       RCurl_1.95-4.8      bitops_1.0-6       \n",
    "[22] dplyr_0.5.0         RSQLite_1.1-2      \n",
    "\n",
    "Notebook:\n",
    "\n",
    "IRdisplay    \"0.4.4\"           \n",
    "IRkernel     \"0.8.6.9000\n",
    "\n",
    "Python version: Python 3.5.2 |\n",
    "Anaconda 4.2.0 (x86_64)\n",
    "csvkit 1.0.2|\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### <a id=\"data-sources\">Data sources:</a>\n",
    "| Data source| Notes|\n",
    "|------------|------|\n",
    "|[aussportbetting.com](http://www.aussportsbetting.com/data/historical-afl-results-and-odds-data/)|for fixtures of all games since 2009|\n",
    "|[wikipedia.org](https://en.wikipedia.org/wiki/List_of_Australian_Football_League_grounds)|for locations and names of the grounds|\n",
    "|[bom.gov.au](http://www.bom.gov.au/) & [google.com.au/maps](https://www.google.com.au/maps)| to work out the nearest weather stations and to download the relevant temperature and rainfall data |\n",
    "|[afltables.com](http://afltables.com/afl/afl_index.html)|for the personal stats on AFL players|\n",
    "|[bigfooty.com](https://www.bigfooty.com/forum/threads/afl-colours-guide.810014/page-2)|AFL team colours manually transcribed from this thread|\n",
    "|[manually created Google Sheet CSV](https://docs.google.com/spreadsheets/d/1tJKEJj4cfXjdgVQvXgkMKZaPzhosNkeJ2GtrgYIINuE/pub?output=csv)|CSV created with team colours from the above thread|\n",
    "|[nfl.com](http://www.nfl.com/teams)|for the personal stats on NFL players|\n",
    "|[zerotackle.com](https://www.zerotackle.com)|for the personal stats on NRL players|\n",
    "|[kaggle.com](https://www.kaggle.com/hugomathien/soccer)|for the personal stats on all European Soccer players|\n",
    "|[besttickets.com](http://www.besttickets.com/blog/wp-content/uploads/2013/12/NBA-Census-10.14.2013.csv)|for the stats on NBA players (2013)|\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
